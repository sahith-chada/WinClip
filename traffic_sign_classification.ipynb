{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schada/miniconda3/envs/venv3.10/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import Modules\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from WinCLIP.model import WinClipAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schada/Desktop/damaged_traffic_sign_detection/WinClip/WinCLIP/CLIPAD/factory.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.grid_size (15, 15)\n",
      "fusion version: textual_visual\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Initialize the Model with Correct Image Sizes\n",
    "out_size_h, out_size_w = 240, 240  # Set to 240 instead of 224\n",
    "backbone = 'ViT-B-16-plus-240'\n",
    "pretrained_dataset = 'laion400m_e32'\n",
    "#scales = [224, 256, 384]  # You can keep the scales as is\n",
    "\n",
    "#scales = [192, 224, 240]  # Adjusted scales\n",
    "scales = [2, 3, 5, 7, 15]  # Example scales that fit within a 15x15 grid\n",
    "\n",
    "precision = 'fp32'  # Use 'fp16' if your GPU supports mixed precision\n",
    "img_resize = 256\n",
    "img_cropsize = 240  # Set to 240 instead of 224\n",
    "\n",
    "model = WinClipAD(\n",
    "    out_size_h=out_size_h,\n",
    "    out_size_w=out_size_w,\n",
    "    device=device,\n",
    "    backbone=backbone,\n",
    "    pretrained_dataset=pretrained_dataset,\n",
    "    scales=scales,\n",
    "    precision=precision,\n",
    "    img_resize=img_resize,\n",
    "    img_cropsize=img_cropsize\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build Text Feature Gallery\n",
    "category = 'traffic sign'\n",
    "model.build_text_feature_gallery(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Prepare Data Transforms with Correct Crop Size\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert('RGB')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_resize, img_resize), Image.BICUBIC),\n",
    "    transforms.CenterCrop(img_cropsize),  # Now crops to 240x240\n",
    "    _convert_to_rgb,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify abnormal prompts in ad_prompts.py (if accessible)\n",
    "# For example, add phrases like 'damaged', 'vandalized', 'graffiti on', 'broken', etc.\n",
    "\n",
    "# Alternatively, directly create custom abnormal phrases\n",
    "abnormal_phrases = [\n",
    "    'A photo of a damaged traffic sign.',\n",
    "    'A picture of a vandalized traffic sign.',\n",
    "    'An image of a traffic sign with graffiti.',\n",
    "    'A photo of a broken traffic sign.',\n",
    "    'A picture of a traffic sign covered in stickers.'\n",
    "]\n",
    "# Define additional abnormal phrases based on your categories\n",
    "additional_abnormal_phrases = [\n",
    "    'A picture of a faded traffic sign.',\n",
    "    'A photo of a traffic sign with graffiti.',\n",
    "    'A photo of a vandalized traffic sign.',\n",
    "    'An image of a traffic sign with other types of vandalism.'\n",
    "    'Traffic signs which are spray painted'\n",
    "]\n",
    "\n",
    "abnormal_phrases.extend(additional_abnormal_phrases)\n",
    "# Tokenize abnormal phrases\n",
    "abnormal_phrases_tokenized = model.tokenizer(abnormal_phrases).to(device)\n",
    "\n",
    "# Encode abnormal text features\n",
    "abnormal_text_features = model.encode_text(abnormal_phrases_tokenized)\n",
    "abnormal_text_features /= abnormal_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Combine with normal text features\n",
    "# Use existing normal_phrases from the model or define similarly\n",
    "normal_phrases = [\n",
    "    'A photo of a traffic sign.',\n",
    "    'A clear picture of a traffic sign.',\n",
    "    'An image of an intact traffic sign.',\n",
    "    'A photo of a standard traffic sign.',\n",
    "    'A picture of an undamaged traffic sign.'\n",
    "]\n",
    "\n",
    "# Tokenize and encode normal phrases\n",
    "normal_phrases_tokenized = model.tokenizer(normal_phrases).to(device)\n",
    "normal_text_features = model.encode_text(normal_phrases_tokenized)\n",
    "normal_text_features /= normal_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute average text features\n",
    "model.avr_normal_text_features = normal_text_features.mean(dim=0, keepdim=True)\n",
    "model.avr_abnormal_text_features = abnormal_text_features.mean(dim=0, keepdim=True)\n",
    "\n",
    "# Update text features in the model\n",
    "model.text_features = torch.cat([\n",
    "    model.avr_normal_text_features,\n",
    "    model.avr_abnormal_text_features\n",
    "], dim=0)\n",
    "model.text_features /= model.text_features.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(256, 256), interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(240, 240))\n",
       "    <function _convert_to_rgb at 0x7f4d0eee8820>\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to normal images\n",
    "normal_image_paths = glob.glob('/home/schada/Desktop/damaged_traffic_sign_detection/data/good_examples/*.*')\n",
    "#additional_normal_image_paths = glob.glob('/path/to/your/additional/normal/images/*.*')\n",
    "\n",
    "# Combine all normal image paths\n",
    "all_normal_image_paths = normal_image_paths #+ additional_normal_image_paths\n",
    "\n",
    "# Transform and load images\n",
    "normal_images = []\n",
    "for img_path in all_normal_image_paths:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')  # Ensure RGB mode\n",
    "        img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "        normal_images.append(img_tensor)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")  # Handle any image loading issues\n",
    "\n",
    "# Concatenate and build feature gallery\n",
    "if normal_images:  # Check if the list is not empty\n",
    "    normal_images_tensor = torch.cat(normal_images, dim=0).to(device)\n",
    "    model.build_image_feature_gallery(normal_images_tensor)\n",
    "else:\n",
    "    print(\"No valid normal images found to build the feature gallery.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1008\n",
      "Number of normal images: 0\n",
      "Number of damaged images: 1008\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the base directory where your images are stored\n",
    "base_dir = '/home/schada/Desktop/damaged_traffic_sign_detection/data/RDX_cropped _dir'\n",
    "\n",
    "# List of damaged directories\n",
    "damaged_dirs = ['bent', 'broken', 'faded', 'graffiti', 'multiple_signs', 'other_vandalism', 'tilted','vandalized']\n",
    "\n",
    "# Initialize lists for image paths and labels\n",
    "test_image_paths = []\n",
    "test_labels = []\n",
    "\n",
    "# Load normal images\n",
    "normal_image_paths = glob.glob(os.path.join(base_dir, 'undamaged', '*.*'))\n",
    "test_image_paths.extend(normal_image_paths)\n",
    "test_labels.extend([0] * len(normal_image_paths))  # Label 0 for normal images\n",
    "\n",
    "# Load damaged images\n",
    "for damaged_dir in damaged_dirs:\n",
    "    damaged_image_paths = glob.glob(os.path.join(base_dir, damaged_dir, '*.*'))\n",
    "    test_image_paths.extend(damaged_image_paths)\n",
    "    test_labels.extend([1] * len(damaged_image_paths))  # Label 1 for damaged images\n",
    "\n",
    "print(f'Total images: {len(test_image_paths)}')\n",
    "print(f'Number of normal images: {len(normal_image_paths)}')\n",
    "print(f'Number of damaged images: {len(test_image_paths) - len(normal_image_paths)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Custom dataset class\n",
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img_tensor = self.transform(img)\n",
    "            return img_tensor, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None, label  # Handle this case in the DataLoader\n",
    "\n",
    "# Create dataset and dataloader\n",
    "transform = model.transform  # Use the model's transform\n",
    "dataset = TrafficSignDataset(test_image_paths, test_labels, transform)\n",
    "\n",
    "batch_size = 16  # Adjust based on your GPU memory\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Compute anomaly scores\n",
    "anomaly_scores = []\n",
    "true_labels = []\n",
    "\n",
    "model.eval_mode()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch_images, batch_labels = batch\n",
    "        # Remove any None entries\n",
    "        batch_images = [img for img in batch_images if img is not None]\n",
    "        batch_labels = [label for img, label in zip(batch_images, batch_labels) if img is not None]\n",
    "        \n",
    "        if not batch_images:\n",
    "            continue  # Skip if batch is empty\n",
    "\n",
    "        batch_images = torch.stack(batch_images).to(device)\n",
    "        anomaly_maps = model(batch_images)\n",
    "        \n",
    "        # Compute anomaly scores\n",
    "        for anomaly_map in anomaly_maps:\n",
    "            anomaly_score = anomaly_map.mean()\n",
    "            anomaly_scores.append(anomaly_score.item())\n",
    "        \n",
    "        true_labels.extend(batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msahith_chada\u001b[0m (\u001b[33mhonda-sv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/schada/Desktop/damaged_traffic_sign_detection/WinClip/wandb/run-20241206_114730-ilisqu6t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/honda-sv/Winclip%20traffic%20sign%20classification/runs/ilisqu6t' target=\"_blank\">new validationb set</a></strong> to <a href='https://wandb.ai/honda-sv/Winclip%20traffic%20sign%20classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/honda-sv/Winclip%20traffic%20sign%20classification' target=\"_blank\">https://wandb.ai/honda-sv/Winclip%20traffic%20sign%20classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/honda-sv/Winclip%20traffic%20sign%20classification/runs/ilisqu6t' target=\"_blank\">https://wandb.ai/honda-sv/Winclip%20traffic%20sign%20classification/runs/ilisqu6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWinclip traffic sign classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew validationb set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Compute ROC AUC\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m auc_score \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manomaly_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROC AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Log the AUC score to W&B\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:640\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    638\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[1;32m    639\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    649\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    650\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    654\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sklearn/metrics/_base.py:76\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     79\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv3.10/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project=\"Winclip traffic sign classification\", name=\"new validationb set\")\n",
    "\n",
    "# Compute ROC AUC\n",
    "auc_score = roc_auc_score(true_labels, anomaly_scores)\n",
    "print(f'ROC AUC: {auc_score:.4f}')\n",
    "\n",
    "# Log the AUC score to W&B\n",
    "wandb.log({\"ROC AUC\": auc_score})\n",
    "\n",
    "# Determine the optimal threshold (e.g., using Youden's J statistic)\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, anomaly_scores)\n",
    "j_scores = tpr - fpr\n",
    "j_ordered = sorted(zip(j_scores, thresholds))\n",
    "threshold = j_ordered[-1][1]\n",
    "\n",
    "# Predict labels based on the threshold\n",
    "predicted_labels = [1 if score >= threshold else 0 for score in anomaly_scores]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "f1 = f1_score(true_labels, predicted_labels, zero_division=0)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Log metrics to W&B\n",
    "wandb.log({\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1-score\": f1,\n",
    "    \"Confusion Matrix\": cm.tolist(),  # Convert the confusion matrix to a list for logging\n",
    "    \"Optimal Threshold\": threshold\n",
    "})\n",
    "\n",
    "# Optionally, log ROC curve\n",
    "# wandb.log({\n",
    "#     \"ROC Curve\": wandb.plot.roc_curve(true_labels, anomaly_scores)\n",
    "# })\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have been categorized into directories.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create output directories\n",
    "output_dir = \"/home/schada/Desktop/damaged_traffic_sign_detection/WinClip/output_images\"\n",
    "categories = [\"true_positive\", \"false_positive\", \"true_negative\", \"false_negative\"]\n",
    "\n",
    "# Ensure the directories exist\n",
    "for category in categories:\n",
    "    dir_path = os.path.join(output_dir, category)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Iterate through the data again and categorize images\n",
    "for img_path, true_label, pred_label in zip(test_image_paths, true_labels, predicted_labels):\n",
    "    if true_label == 1 and pred_label == 1:\n",
    "        category = \"true_positive\"\n",
    "    elif true_label == 0 and pred_label == 1:\n",
    "        category = \"false_positive\"\n",
    "    elif true_label == 0 and pred_label == 0:\n",
    "        category = \"true_negative\"\n",
    "    elif true_label == 1 and pred_label == 0:\n",
    "        category = \"false_negative\"\n",
    "    else:\n",
    "        continue  # This should not happen if labels are binary (0 or 1)\n",
    "\n",
    "    # Destination path\n",
    "    destination_path = os.path.join(output_dir, category, os.path.basename(img_path))\n",
    "\n",
    "    try:\n",
    "        # Copy the image to the respective category directory\n",
    "        shutil.copy(img_path, destination_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying image {img_path}: {e}\")\n",
    "\n",
    "print(\"Images have been categorized into directories.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
